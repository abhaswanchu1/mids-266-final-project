{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruherJ30TZQH"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "gpt_qa_model = \"gpt-4.1-nano-2025-04-14\"\n",
        "openai.api_key =' '##API KEY\n"
      ],
      "metadata": {
        "id": "EsJfJ4dLGEOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "train_dataset = load_dataset('csv', data_files='PLUE/PLUE-main/data/privacyqa/policy_train_data.csv', delimiter='\\t')\n",
        "test_dataset = load_dataset('csv', data_files='PLUE/PLUE-main/data/privacyqa/policy_test_data.csv', delimiter='\\t')\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "train_dataset = train_dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "validation_dataset = train_dataset[\"test\"]\n",
        "train_dataset = train_dataset[\"train\"]\n",
        "\n",
        "print(\"Training dataset:\", train_dataset)\n",
        "print(\"Validation dataset:\", validation_dataset)\n",
        "print(\"Test dataset:\", test_dataset)\n",
        "\n",
        "# Filter relevant and irrelevant examples from the training dataset\n",
        "relevant_examples = [example for example in train_dataset if example[\"Label\"] == \"Relevant\"]\n",
        "irrelevant_examples = [example for example in train_dataset if example[\"Label\"] == \"Irrelevant\"]\n",
        "\n",
        "print(\"\\nNumber of relevant examples in training data:\", len(relevant_examples))\n",
        "print(\"Number of irrelevant examples in training data:\", len(irrelevant_examples))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-mY9HglZbnn",
        "outputId": "c78c1d56-72af-4e37-ffce-e8186f676a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset: Dataset({\n",
            "    features: ['Folder', 'DocID', 'QueryID', 'SentID', 'Split', 'Query', 'Segment', 'Label'],\n",
            "    num_rows: 166680\n",
            "})\n",
            "Validation dataset: Dataset({\n",
            "    features: ['Folder', 'DocID', 'QueryID', 'SentID', 'Split', 'Query', 'Segment', 'Label'],\n",
            "    num_rows: 18520\n",
            "})\n",
            "Test dataset: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['Folder', 'DocID', 'QueryID', 'SentID', 'Split', 'Query', 'Segment', 'Any_Relevant', 'Ann1', 'Ann2', 'Ann3', 'Ann4', 'Ann5', 'Ann6'],\n",
            "        num_rows: 62150\n",
            "    })\n",
            "})\n",
            "\n",
            "Number of relevant examples in training data: 6434\n",
            "Number of irrelevant examples in training data: 160246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e45264f"
      },
      "source": [
        "# Task\n",
        "Set up and run a 1, 2, and 3-shot learning experiment using a loaded model on a test set, including information about class imbalance in the prompts, and evaluate the performance for each shot number."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuhKF3o-HVsg",
        "outputId": "0015dd86-69ab-40a8-820e-740ddf8cce85"
      },
      "source": [
        "# Randomly select 3 \"Relevant\" examples.\n",
        "\n",
        "num_relevant_examples = len(relevant_examples)\n",
        "# Ensure there are enough relevant examples to sample from\n",
        "if num_relevant_examples < 3:\n",
        "    print(f\"Warning: Only {num_relevant_examples} relevant examples available. Selecting all available relevant examples.\")\n",
        "    relevant_indices = np.arange(num_relevant_examples).tolist()\n",
        "else:\n",
        "    relevant_indices = np.random.choice(num_relevant_examples, size=3, replace=False).tolist() # Convert numpy array to list\n",
        "selected_relevant_examples = [\n",
        "    {\"Segment\": relevant_examples[i][\"Segment\"], \"Label\": relevant_examples[i][\"Label\"]}\n",
        "    for i in relevant_indices\n",
        "]\n",
        "\n",
        "# Randomly select 3 \"Irrelevant\" examples.\n",
        "num_irrelevant_examples = len(irrelevant_examples)\n",
        "# Ensure there are enough irrelevant examples to sample from\n",
        "if num_irrelevant_examples < 3:\n",
        "    print(f\"Warning: Only {num_irrelevant_examples} irrelevant examples available. Selecting all available irrelevant examples.\")\n",
        "    irrelevant_indices = np.arange(num_irrelevant_examples).tolist()\n",
        "else:\n",
        "    irrelevant_indices = np.random.choice(num_irrelevant_examples, size=3, replace=False).tolist() # Convert numpy array to list\n",
        "selected_irrelevant_examples = [\n",
        "    {\"Segment\": irrelevant_examples[i][\"Segment\"], \"Label\": irrelevant_examples[i][\"Label\"]}\n",
        "    for i in irrelevant_indices\n",
        "]\n",
        "\n",
        "# Store the selected examples as a list of dictionaries.\n",
        "few_shot_examples = {\n",
        "    \"Relevant\": selected_relevant_examples,\n",
        "    \"Irrelevant\": selected_irrelevant_examples,\n",
        "}\n",
        "\n",
        "print(\"Selected few-shot examples:\")\n",
        "print(few_shot_examples)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected few-shot examples:\n",
            "{'Relevant': [{'Segment': 'Reddit allows other websites to embed public Reddit content via our embed tools.', 'Label': 'Relevant'}, {'Segment': 'We will learn and collect your own wording from your input text messages (SMS or MMS), to help us provide you a faster and more precise prediction (including the popup smiley prediction).', 'Label': 'Relevant'}, {'Segment': 'Deleting Your Account', 'Label': 'Relevant'}], 'Irrelevant': [{'Segment': 'In addition, we are required to list the data access right available to all users in a specific way.', 'Label': 'Irrelevant'}, {'Segment': 'Provide the Service: Your information will be used primarily to provide you with the Service.', 'Label': 'Irrelevant'}, {'Segment': 'maintain appropriate records for internal administrative purposes;', 'Label': 'Irrelevant'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29f707da"
      },
      "source": [
        "## Construct prompts for few-shot learning\n",
        "\n",
        "### Subtask:\n",
        "Create prompts for each test data point, including the selected few-shot examples and the test sentence. Add a note about the class imbalance in the prompt.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "918d27a1",
        "outputId": "80ce7e13-48c3-4275-dbca-226b0d8d5294"
      },
      "source": [
        "def construct_few_shot_prompt(example, few_shot_examples, num_shots, system_instruction=None):\n",
        "    \"\"\"\n",
        "    Constructs a few-shot prompt for a given example.\n",
        "\n",
        "    Args:\n",
        "        example (dict): The test example to classify.\n",
        "        few_shot_examples (dict): A dictionary containing relevant and irrelevant few-shot examples.\n",
        "        num_shots (int): The number of few-shot examples to include in the prompt.\n",
        "        system_instruction (str, optional): An instruction to prepend to the prompt. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        str: The constructed prompt string.\n",
        "    \"\"\"\n",
        "    prompt = \"\"\n",
        "    if system_instruction:\n",
        "        prompt += f\"{system_instruction}\\n\\n\"\n",
        "\n",
        "    # Add few-shot examples\n",
        "    # Include an equal number of relevant and irrelevant examples if possible\n",
        "    # If num_shots is odd, include one more of one class (e.g., relevant)\n",
        "    num_relevant_to_include = num_shots // 2 + (num_shots % 2)\n",
        "    num_irrelevant_to_include = num_shots // 2\n",
        "\n",
        "\n",
        "    # Ensure we don't request more examples than available\n",
        "    num_relevant_to_include = min(num_relevant_to_include, len(few_shot_examples.get(\"Relevant\", [])))\n",
        "    num_irrelevant_to_include = min(num_irrelevant_to_include, len(few_shot_examples.get(\"Irrelevant\", [])))\n",
        "\n",
        "    # Add relevant examples\n",
        "    for i in range(num_relevant_to_include):\n",
        "        relevant_example = few_shot_examples[\"Relevant\"][i]\n",
        "        prompt += f\"Sentence: {relevant_example['Segment']}\\nLabel: {relevant_example['Label']}\\n\\n\"\n",
        "\n",
        "    # Add irrelevant examples\n",
        "    for i in range(num_irrelevant_to_include):\n",
        "         irrelevant_example = few_shot_examples[\"Irrelevant\"][i]\n",
        "         prompt += f\"Sentence: {irrelevant_example['Segment']}\\nLabel: {irrelevant_example['Label']}\\n\\n\"\n",
        "\n",
        "\n",
        "    # Add note about class imbalance\n",
        "    prompt += \"Note: The dataset has a significant class imbalance, with many more 'Irrelevant' examples than 'Relevant' ones.\\n\\n\"\n",
        "\n",
        "\n",
        "    # Add the test sentence\n",
        "    prompt += f\"Sentence: {example['Segment']}\\nLabel:\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Example usage with 1-shot, 2-shot, and 3-shot\n",
        "system_instruction = \"Act as a legal expert and classify the following sentences as 'Relevant' or 'Irrelevant' to privacy policies.\"\n",
        "\n",
        "# Assuming 'test_dataset' is your test dataset loaded\n",
        "# and 'few_shot_examples' is your dictionary of selected few-shot examples\n",
        "\n",
        "# Create prompts for the first test example for demonstration\n",
        "# Access the 'train' split of the test_dataset and get the first example\n",
        "first_test_example = test_dataset['train'][0]\n",
        "\n",
        "prompt_1_shot = construct_few_shot_prompt(first_test_example, few_shot_examples, num_shots=1, system_instruction=system_instruction)\n",
        "prompt_2_shots = construct_few_shot_prompt(first_test_example, few_shot_examples, num_shots=2, system_instruction=system_instruction)\n",
        "prompt_3_shots = construct_few_shot_prompt(first_test_example, few_shot_examples, num_shots=3, system_instruction=system_instruction)\n",
        "\n",
        "print(\"--- 1-Shot Prompt Example ---\")\n",
        "print(prompt_1_shot)\n",
        "print(\"\\n--- 2-Shot Prompt Example ---\")\n",
        "print(prompt_2_shots)\n",
        "print(\"\\n--- 3-Shot Prompt Example ---\")\n",
        "print(prompt_3_shots)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1-Shot Prompt Example ---\n",
            "Act as a legal expert and classify the following sentences as 'Relevant' or 'Irrelevant' to privacy policies.\n",
            "\n",
            "Sentence: Reddit allows other websites to embed public Reddit content via our embed tools.\n",
            "Label: Relevant\n",
            "\n",
            "Note: The dataset has a significant class imbalance, with many more 'Irrelevant' examples than 'Relevant' ones.\n",
            "\n",
            "Sentence:   At Fiverr we care about your privacy.\n",
            "Label:\n",
            "\n",
            "--- 2-Shot Prompt Example ---\n",
            "Act as a legal expert and classify the following sentences as 'Relevant' or 'Irrelevant' to privacy policies.\n",
            "\n",
            "Sentence: Reddit allows other websites to embed public Reddit content via our embed tools.\n",
            "Label: Relevant\n",
            "\n",
            "Sentence: In addition, we are required to list the data access right available to all users in a specific way.\n",
            "Label: Irrelevant\n",
            "\n",
            "Note: The dataset has a significant class imbalance, with many more 'Irrelevant' examples than 'Relevant' ones.\n",
            "\n",
            "Sentence:   At Fiverr we care about your privacy.\n",
            "Label:\n",
            "\n",
            "--- 3-Shot Prompt Example ---\n",
            "Act as a legal expert and classify the following sentences as 'Relevant' or 'Irrelevant' to privacy policies.\n",
            "\n",
            "Sentence: Reddit allows other websites to embed public Reddit content via our embed tools.\n",
            "Label: Relevant\n",
            "\n",
            "Sentence: We will learn and collect your own wording from your input text messages (SMS or MMS), to help us provide you a faster and more precise prediction (including the popup smiley prediction).\n",
            "Label: Relevant\n",
            "\n",
            "Sentence: In addition, we are required to list the data access right available to all users in a specific way.\n",
            "Label: Irrelevant\n",
            "\n",
            "Note: The dataset has a significant class imbalance, with many more 'Irrelevant' examples than 'Relevant' ones.\n",
            "\n",
            "Sentence:   At Fiverr we care about your privacy.\n",
            "Label:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65f87038"
      },
      "source": [
        "# Task\n",
        "Perform 1, 2, and 3-shot learning experiments on the test dataset using the `gpt-4.1-nano` model, construct prompts without explicit tokenization, and evaluate the results using Precision, Recall, and F1-score, considering the class imbalance in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "664375ee",
        "outputId": "b146a384-e081-4f43-dbeb-06216ba7417e"
      },
      "source": [
        "# Get 1-shot predictions for a subset of the test dataset\n",
        "one_shot_predictions_subset = []\n",
        "subset_size = 1000\n",
        "test_subset = test_dataset['train'].select(range(min(subset_size, len(test_dataset['train']))))\n",
        "\n",
        "\n",
        "print(f\"Starting 1-shot predictions for a subset of {len(test_subset)} examples...\")\n",
        "for i in range(len(test_subset)):\n",
        "    if (i + 1) % 100 == 0: # Print progress more frequently for a smaller subset\n",
        "        print(f\"Processing 1-shot example {i + 1}/{len(test_subset)}...\")\n",
        "\n",
        "    example = test_subset[i]\n",
        "    prompt = construct_few_shot_prompt(example, few_shot_examples, num_shots=1, system_instruction=system_instruction)\n",
        "\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=gpt_qa_model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        predicted_label = response.choices[0].message.content.strip()\n",
        "        one_shot_predictions_subset.append(predicted_label)\n",
        "\n",
        "    except Exception as e:\n",
        "        sent_id = example.get('SentID', 'UnknownSentID') if isinstance(example, dict) else 'UnknownSentID'\n",
        "        print(f\"Error getting 1-shot prediction for example with SentID: {sent_id}. Error: {e}\")\n",
        "        one_shot_predictions_subset.append(\"Error\")\n",
        "\n",
        "print(\"Finished getting 1-shot predictions for the subset.\")\n",
        "print(\"Number of 1-shot predictions:\", len(one_shot_predictions_subset))\n",
        "\n",
        "# Proceed to get 2-shot predictions for the subset\n",
        "two_shot_predictions_subset = []\n",
        "\n",
        "print(f\"\\nStarting 2-shot predictions for a subset of {len(test_subset)} examples...\")\n",
        "for i in range(len(test_subset)):\n",
        "    if (i + 1) % 100 == 0: # Print progress more frequently for a smaller subset\n",
        "        print(f\"Processing 2-shot example {i + 1}/{len(test_subset)}...\")\n",
        "\n",
        "    example = test_subset[i]\n",
        "    prompt = construct_few_shot_prompt(example, few_shot_examples, num_shots=2, system_instruction=system_instruction)\n",
        "\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=gpt_qa_model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        predicted_label = response.choices[0].message.content.strip()\n",
        "        two_shot_predictions_subset.append(predicted_label)\n",
        "\n",
        "    except Exception as e:\n",
        "        sent_id = example.get('SentID', 'UnknownSentID') if isinstance(example, dict) else 'UnknownSentID'\n",
        "        print(f\"Error getting 2-shot prediction for example with SentID: {sent_id}. Error: {e}\")\n",
        "        two_shot_predictions_subset.append(\"Error\")\n",
        "\n",
        "print(\"Finished getting 2-shot predictions for the subset.\")\n",
        "print(\"Number of 2-shot predictions:\", len(two_shot_predictions_subset))\n",
        "\n",
        "# Proceed to get 3-shot predictions for the subset\n",
        "three_shot_predictions_subset = []\n",
        "\n",
        "print(f\"\\nStarting 3-shot predictions for a subset of {len(test_subset)} examples...\")\n",
        "for i in range(len(test_subset)):\n",
        "    if (i + 1) % 100 == 0: # Print progress more frequently for a smaller subset\n",
        "        print(f\"Processing 3-shot example {i + 1}/{len(test_subset)}...\")\n",
        "\n",
        "    example = test_subset[i]\n",
        "    prompt = construct_few_shot_prompt(example, few_shot_examples, num_shots=3, system_instruction=system_instruction)\n",
        "\n",
        "    try:\n",
        "        response = openai.chat.completions.create(\n",
        "            model=gpt_qa_model,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "        )\n",
        "        predicted_label = response.choices[0].message.content.strip()\n",
        "        three_shot_predictions_subset.append(predicted_label)\n",
        "\n",
        "    except Exception as e:\n",
        "        sent_id = example.get('SentID', 'UnknownSentID') if isinstance(example, dict) else 'UnknownSentID'\n",
        "        print(f\"Error getting 3-shot prediction for example with SentID: {sent_id}. Error: {e}\")\n",
        "        three_shot_predictions_subset.append(\"Error\")\n",
        "\n",
        "print(\"Finished getting 3-shot predictions for the subset.\")\n",
        "print(\"Number of 3-shot predictions:\", len(three_shot_predictions_subset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting 1-shot predictions for a subset of 1000 examples...\n",
            "Processing 1-shot example 100/1000...\n",
            "Processing 1-shot example 200/1000...\n",
            "Processing 1-shot example 300/1000...\n",
            "Processing 1-shot example 400/1000...\n",
            "Processing 1-shot example 500/1000...\n",
            "Processing 1-shot example 600/1000...\n",
            "Processing 1-shot example 700/1000...\n",
            "Processing 1-shot example 800/1000...\n",
            "Processing 1-shot example 900/1000...\n",
            "Processing 1-shot example 1000/1000...\n",
            "Finished getting 1-shot predictions for the subset.\n",
            "Number of 1-shot predictions: 1000\n",
            "\n",
            "Starting 2-shot predictions for a subset of 1000 examples...\n",
            "Processing 2-shot example 100/1000...\n",
            "Processing 2-shot example 200/1000...\n",
            "Processing 2-shot example 300/1000...\n",
            "Processing 2-shot example 400/1000...\n",
            "Processing 2-shot example 500/1000...\n",
            "Processing 2-shot example 600/1000...\n",
            "Processing 2-shot example 700/1000...\n",
            "Processing 2-shot example 800/1000...\n",
            "Processing 2-shot example 900/1000...\n",
            "Processing 2-shot example 1000/1000...\n",
            "Finished getting 2-shot predictions for the subset.\n",
            "Number of 2-shot predictions: 1000\n",
            "\n",
            "Starting 3-shot predictions for a subset of 1000 examples...\n",
            "Processing 3-shot example 100/1000...\n",
            "Processing 3-shot example 200/1000...\n",
            "Processing 3-shot example 300/1000...\n",
            "Processing 3-shot example 400/1000...\n",
            "Processing 3-shot example 500/1000...\n",
            "Processing 3-shot example 600/1000...\n",
            "Processing 3-shot example 700/1000...\n",
            "Processing 3-shot example 800/1000...\n",
            "Processing 3-shot example 900/1000...\n",
            "Processing 3-shot example 1000/1000...\n",
            "Finished getting 3-shot predictions for the subset.\n",
            "Number of 3-shot predictions: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b5272b6",
        "outputId": "9153ba55-04e7-4361-a6a0-695f51499e2f"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Prepare the ground truth labels for the test subset\n",
        "# We need to get the 'Any_Relevant' labels from the test_subset and convert them to numerical format (0 and 1)\n",
        "# The convert_labels_to_int function expects a 'Label' column, so we'll\n",
        "# temporarily rename 'Any_Relevant' to 'Label' for the mapping.\n",
        "\n",
        "# Check unique values in 'Any_Relevant' of the subset to be sure they are consistent\n",
        "unique_any_relevant_subset_values = set(test_subset['Any_Relevant'])\n",
        "print(\"Unique values in 'Any_Relevant' of the subset:\", unique_any_relevant_subset_values)\n",
        "\n",
        "if unique_any_relevant_subset_values.issubset({\"Irrelevant\", \"Relevant\"}):\n",
        "    # Temporarily rename 'Any_Relevant' to 'Label' for the mapping function\n",
        "    test_subset_with_label = test_subset.rename_column(\"Any_Relevant\", \"Label\")\n",
        "\n",
        "    # Apply the conversion function to get numerical labels\n",
        "    # We can reuse the convert_labels_to_int function defined earlier\n",
        "    def convert_labels_to_int_subset(examples):\n",
        "        label_map = {\"Irrelevant\": 0, \"Relevant\": 1}\n",
        "        return {\"labels\": [label_map[label] for label in examples[\"Label\"]]}\n",
        "\n",
        "    test_subset_with_numerical_labels = test_subset_with_label.map(convert_labels_to_int_subset, batched=True)\n",
        "\n",
        "    # Extract the numerical ground truth labels\n",
        "    ground_truth_labels_subset = test_subset_with_numerical_labels['labels']\n",
        "\n",
        "    print(\"\\nGround truth labels prepared for the subset.\")\n",
        "    print(\"First 10 ground truth labels:\", ground_truth_labels_subset[:10])\n",
        "\n",
        "    # Convert predicted labels to numerical format (0 and 1)\n",
        "    label_map = {\"Irrelevant\": 0, \"Relevant\": 1}\n",
        "    # Handle potential 'Error' predictions by mapping them to a default or excluding them\n",
        "    # Here, we'll map 'Error' to -1 or some other indicator and filter them out for metric computation\n",
        "    def convert_predictions_to_int(predictions, label_map):\n",
        "        numerical_predictions = []\n",
        "        valid_ground_truth_labels = []\n",
        "        for i, pred in enumerate(predictions):\n",
        "            if pred in label_map:\n",
        "                numerical_predictions.append(label_map[pred])\n",
        "                valid_ground_truth_labels.append(ground_truth_labels_subset[i])\n",
        "            else:\n",
        "                # Optionally handle 'Error' or unexpected predictions, e.g., skip them\n",
        "                print(f\"Skipping prediction '{pred}' at index {i} due to unexpected value.\")\n",
        "        return numerical_predictions, valid_ground_truth_labels\n",
        "\n",
        "\n",
        "    # Convert predictions for each shot number\n",
        "    one_shot_numerical_predictions, one_shot_eval_labels = convert_predictions_to_int(one_shot_predictions_subset, label_map)\n",
        "    two_shot_numerical_predictions, two_shot_eval_labels = convert_predictions_to_int(two_shot_predictions_subset, label_map)\n",
        "    three_shot_numerical_predictions, three_shot_eval_labels = convert_predictions_to_int(three_shot_predictions_subset, label_map)\n",
        "\n",
        "\n",
        "    # --- Evaluate 1-Shot Performance ---\n",
        "    if one_shot_numerical_predictions:\n",
        "        print(\"\\n--- 1-Shot Evaluation Results ---\")\n",
        "        # Use zero_division parameter to handle cases where there are no predicted samples for a class\n",
        "        one_shot_precision = precision_score(one_shot_eval_labels, one_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        one_shot_recall = recall_score(one_shot_eval_labels, one_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        one_shot_f1 = f1_score(one_shot_eval_labels, one_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        one_shot_accuracy = accuracy_score(one_shot_eval_labels, one_shot_numerical_predictions)\n",
        "\n",
        "        print(f\"Precision: {one_shot_precision:.4f}\")\n",
        "        print(f\"Recall: {one_shot_recall:.4f}\")\n",
        "        print(f\"F1-score: {one_shot_f1:.4f}\")\n",
        "        print(f\"Accuracy: {one_shot_accuracy:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n--- 1-Shot Evaluation Results ---\")\n",
        "        print(\"No valid predictions to evaluate.\")\n",
        "\n",
        "\n",
        "    # --- Evaluate 2-Shot Performance ---\n",
        "    if two_shot_numerical_predictions:\n",
        "        print(\"\\n--- 2-Shot Evaluation Results ---\")\n",
        "        two_shot_precision = precision_score(two_shot_eval_labels, two_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        two_shot_recall = recall_score(two_shot_eval_labels, two_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        two_shot_f1 = f1_score(two_shot_eval_labels, two_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        two_shot_accuracy = accuracy_score(two_shot_eval_labels, two_shot_numerical_predictions)\n",
        "\n",
        "        print(f\"Precision: {two_shot_precision:.4f}\")\n",
        "        print(f\"Recall: {two_shot_recall:.4f}\")\n",
        "        print(f\"F1-score: {two_shot_f1:.4f}\")\n",
        "        print(f\"Accuracy: {two_shot_accuracy:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n--- 2-Shot Evaluation Results ---\")\n",
        "        print(\"No valid predictions to evaluate.\")\n",
        "\n",
        "    # --- Evaluate 3-Shot Performance ---\n",
        "    if three_shot_numerical_predictions:\n",
        "        print(\"\\n--- 3-Shot Evaluation Results ---\")\n",
        "        three_shot_precision = precision_score(three_shot_eval_labels, three_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        three_shot_recall = recall_score(three_shot_eval_labels, three_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        three_shot_f1 = f1_score(three_shot_eval_labels, three_shot_numerical_predictions, average='weighted', zero_division=0)\n",
        "        three_shot_accuracy = accuracy_score(three_shot_eval_labels, three_shot_numerical_predictions)\n",
        "\n",
        "        print(f\"Precision: {three_shot_precision:.4f}\")\n",
        "        print(f\"Recall: {three_shot_recall:.4f}\")\n",
        "        print(f\"F1-score: {three_shot_f1:.4f}\")\n",
        "        print(f\"Accuracy: {three_shot_accuracy:.4f}\")\n",
        "    else:\n",
        "        print(\"\\n--- 3-Shot Evaluation Results ---\")\n",
        "        print(\"No valid predictions to evaluate.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n'Any_Relevant' column in the subset contains unexpected values. Cannot convert to numerical labels for evaluation.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique values in 'Any_Relevant' of the subset: {'Irrelevant', 'Relevant'}\n",
            "\n",
            "Ground truth labels prepared for the subset.\n",
            "First 10 ground truth labels: [0, 0, 0, 1, 0, 1, 0, 0, 1, 0]\n",
            "Skipping prediction 'Label: Relevant' at index 901 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 29 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 148 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 150 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 166 due to unexpected value.\n",
            "Skipping prediction 'Label: Irrelevant' at index 230 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 239 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 276 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 341 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 364 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 394 due to unexpected value.\n",
            "Skipping prediction 'Label: Irrelevant' at index 403 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 576 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 680 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 740 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 776 due to unexpected value.\n",
            "Skipping prediction 'Label: Irrelevant' at index 811 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 881 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 892 due to unexpected value.\n",
            "Skipping prediction 'The sentence discusses improving services and customizing browsing experiences, with references to user interests and fundamental rights, but it does not directly address privacy policies, data collection, data sharing, or user information management.\n",
            "\n",
            "Label: Irrelevant' at index 912 due to unexpected value.\n",
            "Skipping prediction 'Label: Relevant' at index 32 due to unexpected value.\n",
            "\n",
            "--- 1-Shot Evaluation Results ---\n",
            "Precision: 0.9577\n",
            "Recall: 0.2262\n",
            "F1-score: 0.3180\n",
            "Accuracy: 0.2262\n",
            "\n",
            "--- 2-Shot Evaluation Results ---\n",
            "Precision: 0.9623\n",
            "Recall: 0.2671\n",
            "F1-score: 0.3715\n",
            "Accuracy: 0.2671\n",
            "\n",
            "--- 3-Shot Evaluation Results ---\n",
            "Precision: 0.9540\n",
            "Recall: 0.2523\n",
            "F1-score: 0.3546\n",
            "Accuracy: 0.2523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XaK78hlUkHy6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f487eb73",
        "outputId": "d8fec55d-8d9f-4e86-f67e-a47150cecf41"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Assuming 'one_shot_numerical_predictions', 'two_shot_numerical_predictions',\n",
        "# 'three_shot_numerical_predictions', and their corresponding\n",
        "# 'one_shot_eval_labels', 'two_shot_eval_labels', 'three_shot_eval_labels'\n",
        "# are available from the previous execution and contain only valid predictions/labels.\n",
        "\n",
        "label_names = [\"Irrelevant\", \"Relevant\"]\n",
        "\n",
        "# --- Evaluate 1-Shot Class-wise Performance ---\n",
        "if one_shot_numerical_predictions:\n",
        "    print(\"\\n--- 1-Shot Class-wise Evaluation Results ---\")\n",
        "    # Use zero_division parameter to handle cases where there are no predicted samples for a class\n",
        "    one_shot_precision_classwise = precision_score(one_shot_eval_labels, one_shot_numerical_predictions, average=None, zero_division=0)\n",
        "    one_shot_recall_classwise = recall_score(one_shot_eval_labels, one_shot_numerical_predictions, average=None, zero_division=0)\n",
        "    one_shot_f1_classwise = f1_score(one_shot_eval_labels, one_shot_numerical_predictions, average=None, zero_division=0)\n",
        "\n",
        "    for i, label_name in enumerate(label_names):\n",
        "        print(f\"  {label_name}:\")\n",
        "        print(f\"    Precision: {one_shot_precision_classwise[i]:.4f}\")\n",
        "        print(f\"    Recall: {one_shot_recall_classwise[i]:.4f}\")\n",
        "        print(f\"    F1-score: {one_shot_f1_classwise[i]:.4f}\")\n",
        "else:\n",
        "    print(\"\\n--- 1-Shot Class-wise Evaluation Results ---\")\n",
        "    print(\"No valid predictions to evaluate class-wise.\")\n",
        "\n",
        "\n",
        "# --- Evaluate 2-Shot Class-wise Performance ---\n",
        "if two_shot_numerical_predictions:\n",
        "    print(\"\\n--- 2-Shot Class-wise Evaluation Results ---\")\n",
        "    two_shot_precision_classwise = precision_score(two_shot_eval_labels, two_shot_numerical_predictions, average=None, zero_division=0)\n",
        "    two_shot_recall_classwise = recall_score(two_shot_eval_labels, two_shot_numerical_predictions, average=None, zero_division=0)\n",
        "    two_shot_f1_classwise = f1_score(two_shot_eval_labels, two_shot_numerical_predictions, average=None, zero_division=0)\n",
        "\n",
        "    for i, label_name in enumerate(label_names):\n",
        "        print(f\"  {label_name}:\")\n",
        "        print(f\"    Precision: {two_shot_precision_classwise[i]:.4f}\")\n",
        "        print(f\"    Recall: {two_shot_recall_classwise[i]:.4f}\")\n",
        "        print(f\"    F1-score: {two_shot_f1_classwise[i]:.4f}\")\n",
        "else:\n",
        "    print(\"\\n--- 2-Shot Class-wise Evaluation Results ---\")\n",
        "    print(\"No valid predictions to evaluate class-wise.\")\n",
        "\n",
        "# --- Evaluate 3-Shot Class-wise Performance ---\n",
        "if three_shot_numerical_predictions:\n",
        "    print(\"\\n--- 3-Shot Class-wise Evaluation Results ---\")\n",
        "    three_shot_precision_classwise = precision_score(three_shot_eval_labels, three_shot_numerical_predictions, average=None, zero_division=0)\n",
        "    three_shot_recall_classwise = recall_score(three_shot_eval_labels, three_shot_numerical_predictions, average=None, zero_division=0)\n",
        "    three_shot_f1_classwise = f1_score(three_shot_eval_labels, three_shot_numerical_predictions, average=None, zero_division=0)\n",
        "\n",
        "    for i, label_name in enumerate(label_names):\n",
        "        print(f\"  {label_name}:\")\n",
        "        print(f\"    Precision: {three_shot_precision_classwise[i]:.4f}\")\n",
        "        print(f\"    Recall: {three_shot_recall_classwise[i]:.4f}\")\n",
        "        print(f\"    F1-score: {three_shot_f1_classwise[i]:.4f}\")\n",
        "else:\n",
        "    print(\"\\n--- 3-Shot Class-wise Evaluation Results ---\")\n",
        "    print(\"No valid predictions to evaluate class-wise.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1-Shot Class-wise Evaluation Results ---\n",
            "  Irrelevant:\n",
            "    Precision: 0.9947\n",
            "    Recall: 0.1958\n",
            "    F1-score: 0.3272\n",
            "  Relevant:\n",
            "    Precision: 0.0469\n",
            "    Recall: 0.9744\n",
            "    F1-score: 0.0895\n",
            "\n",
            "--- 2-Shot Class-wise Evaluation Results ---\n",
            "  Irrelevant:\n",
            "    Precision: 1.0000\n",
            "    Recall: 0.2367\n",
            "    F1-score: 0.3828\n",
            "  Relevant:\n",
            "    Precision: 0.0515\n",
            "    Recall: 1.0000\n",
            "    F1-score: 0.0979\n",
            "\n",
            "--- 3-Shot Class-wise Evaluation Results ---\n",
            "  Irrelevant:\n",
            "    Precision: 0.9908\n",
            "    Recall: 0.2240\n",
            "    F1-score: 0.3653\n",
            "  Relevant:\n",
            "    Precision: 0.0473\n",
            "    Recall: 0.9487\n",
            "    F1-score: 0.0901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ef8dafd"
      },
      "source": [
        "## Experiment Details\n",
        "\n",
        "This section outlines the methodology employed for conducting a few-shot learning experiment to classify sentences from privacy policies as 'Relevant' or 'Irrelevant'.\n",
        "\n",
        "**Task:** The primary objective was to set up and execute a 1, 2, and 3-shot learning experiment using a pre-loaded language model (`gpt-4.1-nano-2025-04-14`) on a test dataset. A crucial aspect of this experiment was the inclusion of information regarding class imbalance within the prompts to potentially mitigate its impact on model performance. The performance for each shot number was subsequently evaluated using standard classification metrics.\n",
        "\n",
        "**Model:** The experiment utilized the `gpt-4.1-nano-2025-04-14` model accessed via the OpenAI API.\n",
        "\n",
        "**Dataset:** The experiment was conducted on a test dataset, referred to as `test_dataset`, which contains sentences labeled as either 'Relevant' or 'Irrelevant' to privacy policies.\n",
        "\n",
        "**Few-Shot Setup:**\n",
        "For each shot number (1, 2, and 3), a set of few-shot examples was randomly selected from a pool of labeled examples (`relevant_examples` and `irrelevant_examples`). The selection aimed to include an equal number of 'Relevant' and 'Irrelevant' examples where possible, adjusting for odd shot numbers by including one extra example from one class. These selected examples were used to guide the model's predictions for unseen test sentences.\n",
        "\n",
        "**Prompt Construction:**\n",
        "Prompts were constructed for each test data point. Each prompt included:\n",
        "1.  A system instruction: \"Act as a legal expert and classify the following sentences as 'Relevant' or 'Irrelevant' to privacy policies.\"\n",
        "2.  The selected few-shot examples, formatted as \"Sentence: [sentence]\\nLabel: [label]\".\n",
        "3.  A note explicitly stating the class imbalance in the dataset: \"Note: The dataset has a significant class imbalance, with many more 'Irrelevant' examples than 'Relevant' ones.\"\n",
        "4.  The test sentence to be classified, formatted as \"Sentence: [test sentence]\\nLabel:\".\n",
        "\n",
        "The prompts were constructed without explicit tokenization, relying on the model's inherent processing capabilities.\n",
        "\n",
        "**Experiment Procedure:**\n",
        "For each shot number (1, 2, and 3), the following steps were performed on a subset of the test dataset (1000 examples for evaluation purposes):\n",
        "1.  A prompt was constructed for each example in the test subset using the `construct_few_shot_prompt` function, incorporating the selected few-shot examples and the class imbalance note.\n",
        "2.  The constructed prompt was sent to the `gpt-4.1-nano-2025-04-14` model via the OpenAI API (`openai.chat.completions.create`).\n",
        "3.  The predicted label was extracted from the model's response. The response format was observed to be the predicted label word ('Relevant' or 'Irrelevant').\n",
        "4.  The predicted labels for all examples in the subset were collected for each shot number.\n",
        "\n",
        "**Evaluation:**\n",
        "The performance of the model for each shot number was evaluated using the following metrics:\n",
        "-   **Precision:** The ratio of correctly predicted 'Relevant' instances to the total predicted 'Relevant' instances.\n",
        "-   **Recall:** The ratio of correctly predicted 'Relevant' instances to the total actual 'Relevant' instances.\n",
        "-   **F1-score:** The harmonic mean of Precision and Recall, providing a balanced measure of the model's performance.\n",
        "-   **Accuracy:** The ratio of correctly predicted instances (both 'Relevant' and 'Irrelevant') to the total number of instances.\n",
        "\n",
        "The ground truth labels from the test subset were converted to a numerical format (0 for 'Irrelevant', 1 for 'Relevant') for metric computation. Predicted labels were also converted to the same numerical format, with unexpected prediction formats being noted and excluded from the evaluation. Weighted averaging was used for Precision, Recall, and F1-score to account for the class imbalance in the evaluation subset.\n",
        "\n",
        "The evaluation results for each shot number (1, 2, and 3) were computed and reported."
      ]
    }
  ]
}